{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f70d756",
   "metadata": {},
   "source": [
    "# DAT405 Assignment 5 - Reinforcement learning and classification\n",
    "## Group 2\n",
    "\n",
    "### Francisco Boudagh\n",
    "### Jakob Engstr√∂m\n",
    "\n",
    "### May 12, 2023\n",
    "\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6486ed",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "## 1. a) Optimal path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58418187",
   "metadata": {},
   "source": [
    "The optimal path is the one with the highest reward.\n",
    "\n",
    "|    |    |    |\n",
    "|----------|----------|----------|\n",
    "|   -1     |    1     |    F     |\n",
    "|    0     |   -1     |    1     |\n",
    "|   -1     |    0     |   -1     |\n",
    "|    S     |   -1     |    1     |\n",
    "\n",
    "\n",
    "\n",
    "To move from $S$ to $F$ we can go $EENNN$ or $EENNWNE$ and both gives a reward $0$, which is the highest score one can get. So the path is not unique since there are two. The first path, $EENNN$ is shorter; 5 steps when the second path, $EENNWNE$ is longer; 7 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fdcb25",
   "metadata": {},
   "source": [
    "## 1. b) Optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4c6c9",
   "metadata": {},
   "source": [
    "Every state could be described by $x$ and $y$ coordinates in the grid. Optimal policy could be obtained by choosing the direction which gives the most reward\n",
    "\n",
    "$[[E, E, -]$\n",
    "\n",
    "$[NSE, NE, N]$\n",
    "\n",
    "$[NE, NSEW, NS]$\n",
    "\n",
    "$[NE, E, NW]]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919fe2c",
   "metadata": {},
   "source": [
    "## 1. c) Reward in a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e9a48",
   "metadata": {},
   "source": [
    "The reward in a) is zero: $-1+1-1+1=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57bb31a",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# Problem 2\n",
    "## 2. a) Iteration algorithm using Bellman equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e23d1632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.61292366 51.94805195 45.61292366]\n",
      " [51.94805195 48.05194805 51.94805195]\n",
      " [45.61292366 51.94805195 45.61292366]]\n",
      "ES, S, SW\n",
      "E, NESW, W\n",
      "NE, N, NW\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def value(states, reward, gamma, epsilon, p):\n",
    "    next_state = np.zeros_like(states)\n",
    "    policy = np.empty(states.shape, dtype=object)\n",
    "\n",
    "    def move(i, j):\n",
    "        m, n = states.shape\n",
    "        for x, y, action in [(i-1, j, 'N'), (i, j+1, 'E'), (i+1, j, 'S'), (i, j-1, 'W')]:\n",
    "            if 0 <= x < m and 0 <= y < n:\n",
    "                yield x, y, action\n",
    "\n",
    "    def rewards(c_current, c_next):\n",
    "        action_reward = p * (reward[c_next] + gamma * next_state[c_next]) + (1-p) * (reward[c_current] + gamma * next_state[c_current])\n",
    "        return action_reward\n",
    "\n",
    "    differ = float('inf')\n",
    "    while differ > epsilon:\n",
    "        differ = 0\n",
    "        for i in range(states.shape[0]):\n",
    "            for j in range(states.shape[1]):\n",
    "                actions = list(move(i, j))\n",
    "                max_reward = float('-inf')\n",
    "                best_actions = []\n",
    "                for action in actions:\n",
    "                    action_reward = rewards((i, j), action[:2])\n",
    "                    if action_reward > max_reward:\n",
    "                        max_reward = action_reward\n",
    "                        best_actions = [action[2]]\n",
    "                    elif action_reward == max_reward:\n",
    "                        best_actions.append(action[2])\n",
    "                next_state[i][j] = max_reward\n",
    "                policy[i][j] = best_actions\n",
    "                differ = max(differ, abs(states[i][j] - next_state[i][j]))\n",
    "        np.copyto(states, next_state)\n",
    "\n",
    "    print(states)\n",
    "    for row in policy:\n",
    "        print(', '.join([''.join(actions) for actions in row]))\n",
    "\n",
    "\n",
    "# testing with given example\n",
    "reward = np.array([[0, 0, 0], [0, 10, 0], [0, 0, 0]], dtype=float)\n",
    "states = np.zeros_like(reward)\n",
    "epsilon = 1e-10\n",
    "gamma = 0.9\n",
    "p = 0.8\n",
    "value(states, reward, gamma, epsilon, p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac7ee1",
   "metadata": {},
   "source": [
    "## 2. b) Independence of the initial value $V_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200c3ab",
   "metadata": {},
   "source": [
    "Because whatever the initial value $V_0$ is, the algorithm (using Bellman equation) will iterively update itself until convergence. And by using the Bellman equation we are converging towards the maximum value. So, analogically, if we have a continuos graphical function, it does not matter where we start on the function if we are aiming toward the maximum value, the starting point will only affect the time for us to get to the maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8052f6d",
   "metadata": {},
   "source": [
    "## 2. c) Discount factor $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d9327",
   "metadata": {},
   "source": [
    "The discount factor $\\gamma$ is a value between $0$ and $1$. It is a factor for how much future rewards are valued. A $\\gamma$ value of $1$ means that the agent is both long term and short term sighted, meaning future rewards are as important as the near-future rewards. When $\\gamma$ is $0$, it means that the agent is short-sighted and will only take an action based on the present rewards.\n",
    "\n",
    "The gamma factor depends on the specific case. For example, an agent that is navigating through a maze shoud definetily have a high $\\gamma$ value to be able to consider future rewards and find the optimal path through the maze. If $\\gamma$ was set to to a small value in maze-case, this could result in a big problem since the agent could easily be stuck in dead end loops and never find a way out of the maze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217029f",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9cbbae",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# Problem 4\n",
    "## 4. a) Importance of exploaration in Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632713e",
   "metadata": {},
   "source": [
    "Exploration in reinforcemnt learning could be seen as trial and error. If no exploration is done, better paths or ways that make the process more effective can not be utilized. An example could be a game of chess. If new to the game, one makes simple moves which probably will lead to a lot of beginners mistakes. If you decide not to play regularly you will make the same mistakes the next time you play. But if you decide to play often (explore) you will learn what moves work the best and as you play different opponents you will gradually be able to use different tactics on different kind of opponents, slowly becoming a master of chess due to exploration, trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62463d05",
   "metadata": {},
   "source": [
    "## 4. b) Reinforcement Learning vs Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c54e05",
   "metadata": {},
   "source": [
    "In supervised learning such as Classification and Regression one has a set of data of which the program learns from and then given a new data set either classifies it based on given data or if given a parameter predicts the associated value from the function from regression. So the goal is to place a given data point in a matching context. In Reinforecment learnin however the agent \"learns\" by trying diferent paths and actions.\n",
    "\n",
    "So the main difference is that in RL the agent explores the environment and learns by the consequenses of actions while in SL the agent is given a dataset for learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea5149",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# Problem 5\n",
    "## 5. a) Decision trees extension to random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b00be3",
   "metadata": {},
   "source": [
    "A decision tree is a set of instructions that helps us make decisions step by step. However a problem with decision trees is that they can become too focused on the specific data they were trained on, which is called overfitting. This means that if we use the same decision tree on a different dataset, it may not work well.\n",
    "\n",
    "To overcome this issue, we can use something called a random forest. In a random foeest, we create multiple decision trees, but each tree only looks at a random part of the dataset. By doing this, the trees are trained a bit differently from each other.\n",
    "\n",
    "The final result of a random forest is obtained by combining the predictions of all the individual trees. This will help to reduce the overfitting problem and provides a more accurate outcome compared to using just one decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e816329",
   "metadata": {},
   "source": [
    "## 5. b) Random forests vs decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d431d",
   "metadata": {},
   "source": [
    "The random forest main advantage is the reduced risk of overfitting, however it is a bit harder to interpret and takes more computational power to construct since there are several trees to be made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
